{
    "problem": "Grandma walks 3 miles every day on her favorite walking trail, which includes 2 miles of walking on the beach and 1 mile of walking on the sidewalk.  On the sidewalk, Grandma walks at twice the rate of speed that she does on the beach.  If 40 minutes of her walk is spent on the beach, how long does it take for her to complete the entire 3-mile walk, in minutes?",
    "solution": "On the beach, Grandma walks at a rate of 2 miles per 40 minutes, or 2/40 = 1/20 miles per minute.\nOn the sidewalk, she walks at twice the rate of speed as when she is on the sand, or 2 * 1/20 = 1/10 miles per minute.\nTo walk 1 mile on the sidewalk, it takes her 1/(1/10) = <<1/(1/10)=10>>10 minutes.\nThus, in total, it takes Grandma 40+10 = <<40+10=50>>50 minutes to walk her favorite route.\n#### 50",
    "tags": [
        "reasoning/math"
    ]
}